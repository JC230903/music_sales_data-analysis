{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2792978,"sourceType":"datasetVersion","datasetId":1211465}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-13T12:23:09.540461Z","iopub.execute_input":"2024-08-13T12:23:09.540764Z","iopub.status.idle":"2024-08-13T12:23:10.652677Z","shell.execute_reply.started":"2024-08-13T12:23:09.540737Z","shell.execute_reply":"2024-08-13T12:23:10.651408Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/billboard-the-hot-100-songs/charts.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyspark\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:27:04.182744Z","iopub.execute_input":"2024-08-13T12:27:04.183157Z","iopub.status.idle":"2024-08-13T12:27:56.543315Z","shell.execute_reply.started":"2024-08-13T12:27:04.183122Z","shell.execute_reply":"2024-08-13T12:27:56.542034Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812366 sha256=743cc3d7ee18f50d0bd28886d36115f15d35534bf9a1011a40f927420a278f67\n  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"BillboardHot100Cube\").getOrCreate()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:28:11.459942Z","iopub.execute_input":"2024-08-13T12:28:11.462526Z","iopub.status.idle":"2024-08-13T12:28:19.366367Z","shell.execute_reply.started":"2024-08-13T12:28:11.462446Z","shell.execute_reply":"2024-08-13T12:28:19.364419Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/08/13 12:28:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the dataset\nfile_path = \"/kaggle/input/billboard-the-hot-100-songs/charts.csv\"\ncharts_df = spark.read.csv(file_path, header=True, inferSchema=True)\n\n# Display the schema of the DataFrame\ncharts_df.printSchema()\n\n# Create a cube to analyze rankings over time and by artist\nfrom pyspark.sql.functions import sum as _sum, count\n\ncube_df = charts_df.groupBy(\"artist\", \"song\", \"date\").agg(\n    _sum(\"rank\").alias(\"TotalRank\"),\n    count(\"song\").alias(\"SongCount\")\n)\n\n# Save the cube DataFrame to a CSV file\noutput_path = \"/kaggle/working/billboard_cube.csv\"\ncube_df.write.format(\"csv\").save(output_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:28:31.961746Z","iopub.execute_input":"2024-08-13T12:28:31.962313Z","iopub.status.idle":"2024-08-13T12:28:48.408506Z","shell.execute_reply.started":"2024-08-13T12:28:31.962255Z","shell.execute_reply":"2024-08-13T12:28:48.407346Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"root\n |-- date: date (nullable = true)\n |-- rank: integer (nullable = true)\n |-- song: string (nullable = true)\n |-- artist: string (nullable = true)\n |-- last-week: string (nullable = true)\n |-- peak-rank: integer (nullable = true)\n |-- weeks-on-board: integer (nullable = true)\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]}]}